weekID = rep(1:length(weekDates),each=7)
weekID = weekID[1:dim(fullDatesDF)[1]]
fullDatesDF$WeekID = weekID
# sum presence in each week
summaryData = fullDatesDF %>%
group_by(WeekID) %>%
summarize(Pres=sum(Pres,na.rm=TRUE))
# normalize by effort
effDF = data.frame(count=rep(1,length(allDates)),weekID=weekID)
effDF$count[which(is.na(fullDatesDF$Pres))] = 0
propEff = effDF %>%
group_by(weekID) %>%
summarize(sum(count))
summaryData$propEff = unlist(propEff[,2])/7
summaryData$Pres = summaryData$Pres*(1/summaryData$propEff)
summaryData$Site = sites[l]
summaryData$WeekDate = weekDates
for (j in 4:(dim(data)[2])){
var = names(data)[j]
# calculate weekly average for this covar
eval(parse(text=paste('thisCovar=fullDatesDF%>%group_by(WeekID)%>%summarize(',var,'=mean(',var,',na.rm=TRUE))',sep="")))
eval(parse(text='summaryData[[var]]=unlist(thisCovar[,2])'))
}
weeklyDF = rbind(weeklyDF,summaryData)
}
masterDF_plusJax = weeklyDF[,c(5,4,10:19)]
masterDF_plusJax$Md = weeklyDF$Pres
spec = 'Cuvier'
outDir = "E:/Chpt_3/GAM_Output"
# if it doesn't already exist, create directory to save models and figures
if (!dir.exists(paste(outDir,'/',spec,sep=""))){
dir.create(paste(outDir,'/',spec,sep=""))
}
data = data.frame(read.csv('Cuvier_masterDF_plusJAX.csv'))
# Round presence to get Poisson dist
data$Pres = round(data$Pres)
data$Date = as.Date(data$Date,origin='1970-01-01')
# create weekly time series to reduce autocorrelation
stDt = as.Date("2016-05-01")
edDt = as.Date("2019-04-30")
sites = c('HZ','OC','NC','BC','WC','NFC','HAT','GS','BP','BS','JAX')
allDates = seq.Date(stDt,edDt,by=1)
weekDates = seq.Date(stDt,edDt,by=7)
weeklyDF = as.numeric()
for (l in 1:length(sites)) {
# Create dataframe to hold data (or NAs) for all dates
fullDatesDF = data.frame(matrix(nrow=length(allDates), ncol=dim(data)[2]))
fullDatesDF[,1] = allDates
# sort the observations we have for this site into the full date range
thisSite = which(!is.na(str_match(data$Site,sites[l])))
matchRow = match(data$Date[thisSite],allDates)
fullDatesDF[matchRow,2:dim(data)[2]] = data[thisSite,2:dim(data)[2]]
colnames(fullDatesDF) = colnames(data)
# create grouping variable
weekID = rep(1:length(weekDates),each=7)
weekID = weekID[1:dim(fullDatesDF)[1]]
fullDatesDF$WeekID = weekID
# sum presence in each week
summaryData = fullDatesDF %>%
group_by(WeekID) %>%
summarize(Pres=sum(Pres,na.rm=TRUE))
# normalize by effort
effDF = data.frame(count=rep(1,length(allDates)),weekID=weekID)
effDF$count[which(is.na(fullDatesDF$Pres))] = 0
propEff = effDF %>%
group_by(weekID) %>%
summarize(sum(count))
summaryData$propEff = unlist(propEff[,2])/7
summaryData$Pres = summaryData$Pres*(1/summaryData$propEff)
summaryData$Site = sites[l]
summaryData$WeekDate = weekDates
for (j in 4:(dim(data)[2])){
var = names(data)[j]
# calculate weekly average for this covar
eval(parse(text=paste('thisCovar=fullDatesDF%>%group_by(WeekID)%>%summarize(',var,'=mean(',var,',na.rm=TRUE))',sep="")))
eval(parse(text='summaryData[[var]]=unlist(thisCovar[,2])'))
}
weeklyDF = rbind(weeklyDF,summaryData)
}
masterDF_plusJax$ZC = weeklyDF$Pres
spec = 'Gervais'
outDir = "E:/Chpt_3/GAM_Output"
# if it doesn't already exist, create directory to save models and figures
if (!dir.exists(paste(outDir,'/',spec,sep=""))){
dir.create(paste(outDir,'/',spec,sep=""))
}
data = data.frame(read.csv('Gervais_masterDF_plusJAX.csv'))
# Round presence to get Poisson dist
data$Pres = round(data$Pres)
data$Date = as.Date(data$Date,origin='1970-01-01')
# create weekly time series to reduce autocorrelation
stDt = as.Date("2016-05-01")
edDt = as.Date("2019-04-30")
sites = c('HZ','OC','NC','BC','WC','NFC','HAT','GS','BP','BS','JAX')
allDates = seq.Date(stDt,edDt,by=1)
weekDates = seq.Date(stDt,edDt,by=7)
weeklyDF = as.numeric()
for (l in 1:length(sites)) {
# Create dataframe to hold data (or NAs) for all dates
fullDatesDF = data.frame(matrix(nrow=length(allDates), ncol=dim(data)[2]))
fullDatesDF[,1] = allDates
# sort the observations we have for this site into the full date range
thisSite = which(!is.na(str_match(data$Site,sites[l])))
matchRow = match(data$Date[thisSite],allDates)
fullDatesDF[matchRow,2:dim(data)[2]] = data[thisSite,2:dim(data)[2]]
colnames(fullDatesDF) = colnames(data)
# create grouping variable
weekID = rep(1:length(weekDates),each=7)
weekID = weekID[1:dim(fullDatesDF)[1]]
fullDatesDF$WeekID = weekID
# sum presence in each week
summaryData = fullDatesDF %>%
group_by(WeekID) %>%
summarize(Pres=sum(Pres,na.rm=TRUE))
# normalize by effort
effDF = data.frame(count=rep(1,length(allDates)),weekID=weekID)
effDF$count[which(is.na(fullDatesDF$Pres))] = 0
propEff = effDF %>%
group_by(weekID) %>%
summarize(sum(count))
summaryData$propEff = unlist(propEff[,2])/7
summaryData$Pres = summaryData$Pres*(1/summaryData$propEff)
summaryData$Site = sites[l]
summaryData$WeekDate = weekDates
for (j in 4:(dim(data)[2])){
var = names(data)[j]
# calculate weekly average for this covar
eval(parse(text=paste('thisCovar=fullDatesDF%>%group_by(WeekID)%>%summarize(',var,'=mean(',var,',na.rm=TRUE))',sep="")))
eval(parse(text='summaryData[[var]]=unlist(thisCovar[,2])'))
}
weeklyDF = rbind(weeklyDF,summaryData)
}
masterDF_plusJax$Me = weeklyDF$Pres
spec = 'Kogia'
outDir = "E:/Chpt_3/GAM_Output"
# if it doesn't already exist, create directory to save models and figures
if (!dir.exists(paste(outDir,'/',spec,sep=""))){
dir.create(paste(outDir,'/',spec,sep=""))
}
data = data.frame(read.csv('Kogia_masterDF_plusJAX.csv'))
# Round presence to get Poisson dist
data$Pres = round(data$Pres)
data$Date = as.Date(data$Date,origin='1970-01-01')
# create weekly time series to reduce autocorrelation
stDt = as.Date("2016-05-01")
edDt = as.Date("2019-04-30")
sites = c('HZ','OC','NC','BC','WC','NFC','HAT','GS','BP','BS','JAX')
allDates = seq.Date(stDt,edDt,by=1)
weekDates = seq.Date(stDt,edDt,by=7)
weeklyDF = as.numeric()
for (l in 1:length(sites)) {
# Create dataframe to hold data (or NAs) for all dates
fullDatesDF = data.frame(matrix(nrow=length(allDates), ncol=dim(data)[2]))
fullDatesDF[,1] = allDates
# sort the observations we have for this site into the full date range
thisSite = which(!is.na(str_match(data$Site,sites[l])))
matchRow = match(data$Date[thisSite],allDates)
fullDatesDF[matchRow,2:dim(data)[2]] = data[thisSite,2:dim(data)[2]]
colnames(fullDatesDF) = colnames(data)
# create grouping variable
weekID = rep(1:length(weekDates),each=7)
weekID = weekID[1:dim(fullDatesDF)[1]]
fullDatesDF$WeekID = weekID
# sum presence in each week
summaryData = fullDatesDF %>%
group_by(WeekID) %>%
summarize(Pres=sum(Pres,na.rm=TRUE))
# normalize by effort
effDF = data.frame(count=rep(1,length(allDates)),weekID=weekID)
effDF$count[which(is.na(fullDatesDF$Pres))] = 0
propEff = effDF %>%
group_by(weekID) %>%
summarize(sum(count))
summaryData$propEff = unlist(propEff[,2])/7
summaryData$Pres = summaryData$Pres*(1/summaryData$propEff)
summaryData$Site = sites[l]
summaryData$WeekDate = weekDates
for (j in 4:(dim(data)[2])){
var = names(data)[j]
# calculate weekly average for this covar
eval(parse(text=paste('thisCovar=fullDatesDF%>%group_by(WeekID)%>%summarize(',var,'=mean(',var,',na.rm=TRUE))',sep="")))
eval(parse(text='summaryData[[var]]=unlist(thisCovar[,2])'))
}
weeklyDF = rbind(weeklyDF,summaryData)
}
masterDF_plusJax$Kg = weeklyDF$Pres
spec = 'Risso'
outDir = "E:/Chpt_3/GAM_Output"
# if it doesn't already exist, create directory to save models and figures
if (!dir.exists(paste(outDir,'/',spec,sep=""))){
dir.create(paste(outDir,'/',spec,sep=""))
}
data = data.frame(read.csv('Risso_masterDF_plusJAX.csv'))
# Round presence to get Poisson dist
data$Pres = round(data$Pres)
data$Date = as.Date(data$Date,origin='1970-01-01')
# create weekly time series to reduce autocorrelation
stDt = as.Date("2016-05-01")
edDt = as.Date("2019-04-30")
sites = c('HZ','OC','NC','BC','WC','NFC','HAT','GS','BP','BS','JAX')
allDates = seq.Date(stDt,edDt,by=1)
weekDates = seq.Date(stDt,edDt,by=7)
weeklyDF = as.numeric()
for (l in 1:length(sites)) {
# Create dataframe to hold data (or NAs) for all dates
fullDatesDF = data.frame(matrix(nrow=length(allDates), ncol=dim(data)[2]))
fullDatesDF[,1] = allDates
# sort the observations we have for this site into the full date range
thisSite = which(!is.na(str_match(data$Site,sites[l])))
matchRow = match(data$Date[thisSite],allDates)
fullDatesDF[matchRow,2:dim(data)[2]] = data[thisSite,2:dim(data)[2]]
colnames(fullDatesDF) = colnames(data)
# create grouping variable
weekID = rep(1:length(weekDates),each=7)
weekID = weekID[1:dim(fullDatesDF)[1]]
fullDatesDF$WeekID = weekID
# Sum presence in each week
summaryData = fullDatesDF %>%
group_by(WeekID) %>%
summarize(Pres=sum(Pres,na.rm=TRUE))
# normalize by effort
effDF = data.frame(count=rep(1,length(allDates)),weekID=weekID)
effDF$count[which(is.na(fullDatesDF$Pres))] = 0
propEff = effDF %>%
group_by(weekID) %>%
summarize(sum(count))
summaryData$propEff = unlist(propEff[,2])/7
summaryData$Pres = summaryData$Pres*(1/summaryData$propEff)
summaryData$Site = sites[l]
summaryData$WeekDate = weekDates
for (j in 4:(dim(data)[2])){
var = names(data)[j]
# calculate weekly average for this covar
eval(parse(text=paste('thisCovar=fullDatesDF%>%group_by(WeekID)%>%summarize(',var,'=mean(',var,',na.rm=TRUE))',sep="")))
eval(parse(text='summaryData[[var]]=unlist(thisCovar[,2])'))
}
weeklyDF = rbind(weeklyDF,summaryData)
}
masterDF_plusJax$Gg = weeklyDF$Pres
spec = 'SBCD'
outDir = "E:/Chpt_3/GAM_Output"
# if it doesn't already exist, create directory to save models and figures
if (!dir.exists(paste(outDir,'/',spec,sep=""))){
dir.create(paste(outDir,'/',spec,sep=""))
}
data = data.frame(read.csv('UD28_masterDF_plusJAX.csv'))
# Round presence to get Poisson dist
data$Pres = round(data$Pres)
data$Date = as.Date(data$Date,origin='1970-01-01')
# create weekly time series to reduce autocorrelation
stDt = as.Date("2016-05-01")
edDt = as.Date("2019-04-30")
sites = c('HZ','OC','NC','BC','WC','NFC','HAT','GS','BP','BS','JAX')
allDates = seq.Date(stDt,edDt,by=1)
weekDates = seq.Date(stDt,edDt,by=7)
weeklyDF = as.numeric()
for (l in 1:length(sites)) {
# Create dataframe to hold data (or NAs) for all dates
fullDatesDF = data.frame(matrix(nrow=length(allDates), ncol=dim(data)[2]))
fullDatesDF[,1] = allDates
# sort the observations we have for this site into the full date range
thisSite = which(!is.na(str_match(data$Site,sites[l])))
matchRow = match(data$Date[thisSite],allDates)
fullDatesDF[matchRow,2:dim(data)[2]] = data[thisSite,2:dim(data)[2]]
colnames(fullDatesDF) = colnames(data)
# create grouping variable
weekID = rep(1:length(weekDates),each=7)
weekID = weekID[1:dim(fullDatesDF)[1]]
fullDatesDF$WeekID = weekID
# Sum presence in each week
summaryData = fullDatesDF %>%
group_by(WeekID) %>%
summarize(Pres=sum(Pres,na.rm=TRUE))
# normalize by effort
effDF = data.frame(count=rep(1,length(allDates)),weekID=weekID)
effDF$count[which(is.na(fullDatesDF$Pres))] = 0
propEff = effDF %>%
group_by(weekID) %>%
summarize(sum(count))
summaryData$propEff = unlist(propEff[,2])/7
summaryData$Pres = summaryData$Pres*(1/summaryData$propEff)
summaryData$Site = sites[l]
summaryData$WeekDate = weekDates
for (j in 4:(dim(data)[2])){
var = names(data)[j]
# calculate weekly average for this covar
eval(parse(text=paste('thisCovar=fullDatesDF%>%group_by(WeekID)%>%summarize(',var,'=mean(',var,',na.rm=TRUE))',sep="")))
eval(parse(text='summaryData[[var]]=unlist(thisCovar[,2])'))
}
weeklyDF = rbind(weeklyDF,summaryData)
}
masterDF_plusJax$Dd = weeklyDF$Pres
spec = 'SFPW'
outDir = "E:/Chpt_3/GAM_Output"
# if it doesn't already exist, create directory to save models and figures
if (!dir.exists(paste(outDir,'/',spec,sep=""))){
dir.create(paste(outDir,'/',spec,sep=""))
}
data = data.frame(read.csv('UD26_masterDF_plusJAX.csv'))
# Round presence to get Poisson dist
data$Pres = round(data$Pres)
data$Date = as.Date(data$Date,origin='1970-01-01')
# create weekly time series to reduce autocorrelation
stDt = as.Date("2016-05-01")
edDt = as.Date("2019-04-30")
sites = c('HZ','OC','NC','BC','WC','NFC','HAT','GS','BP','BS','JAX')
allDates = seq.Date(stDt,edDt,by=1)
weekDates = seq.Date(stDt,edDt,by=7)
weeklyDF = as.numeric()
for (l in 1:length(sites)) {
# Create dataframe to hold data (or NAs) for all dates
fullDatesDF = data.frame(matrix(nrow=length(allDates), ncol=dim(data)[2]))
fullDatesDF[,1] = allDates
# sort the observations we have for this site into the full date range
thisSite = which(!is.na(str_match(data$Site,sites[l])))
matchRow = match(data$Date[thisSite],allDates)
fullDatesDF[matchRow,2:dim(data)[2]] = data[thisSite,2:dim(data)[2]]
colnames(fullDatesDF) = colnames(data)
# create grouping variable
weekID = rep(1:length(weekDates),each=7)
weekID = weekID[1:dim(fullDatesDF)[1]]
fullDatesDF$WeekID = weekID
# Sum presence in each week
summaryData = fullDatesDF %>%
group_by(WeekID) %>%
summarize(Pres=sum(Pres,na.rm=TRUE))
# normalize by effort
effDF = data.frame(count=rep(1,length(allDates)),weekID=weekID)
effDF$count[which(is.na(fullDatesDF$Pres))] = 0
propEff = effDF %>%
group_by(weekID) %>%
summarize(sum(count))
summaryData$propEff = unlist(propEff[,2])/7
summaryData$Pres = summaryData$Pres*(1/summaryData$propEff)
summaryData$Site = sites[l]
summaryData$WeekDate = weekDates
for (j in 4:(dim(data)[2])){
var = names(data)[j]
# calculate weekly average for this covar
eval(parse(text=paste('thisCovar=fullDatesDF%>%group_by(WeekID)%>%summarize(',var,'=mean(',var,',na.rm=TRUE))',sep="")))
eval(parse(text='summaryData[[var]]=unlist(thisCovar[,2])'))
}
weeklyDF = rbind(weeklyDF,summaryData)
}
masterDF_plusJax$Gm = weeklyDF$Pres
spec = 'Sowerby'
outDir = "E:/Chpt_3/GAM_Output"
# if it doesn't already exist, create directory to save models and figures
if (!dir.exists(paste(outDir,'/',spec,sep=""))){
dir.create(paste(outDir,'/',spec,sep=""))
}
data = data.frame(read.csv('Sowerby_masterDF_plusJAX.csv'))
# Round presence to get Poisson dist
data$Pres = round(data$Pres)
data$Date = as.Date(data$Date,origin='1970-01-01')
# create weekly time series to reduce autocorrelation
stDt = as.Date("2016-05-01")
edDt = as.Date("2019-04-30")
sites = c('HZ','OC','NC','BC','WC','NFC','HAT','GS','BP','BS','JAX')
allDates = seq.Date(stDt,edDt,by=1)
weekDates = seq.Date(stDt,edDt,by=7)
weeklyDF = as.numeric()
for (l in 1:length(sites)) {
# Create dataframe to hold data (or NAs) for all dates
fullDatesDF = data.frame(matrix(nrow=length(allDates), ncol=dim(data)[2]))
fullDatesDF[,1] = allDates
# sort the observations we have for this site into the full date range
thisSite = which(!is.na(str_match(data$Site,sites[l])))
matchRow = match(data$Date[thisSite],allDates)
fullDatesDF[matchRow,2:dim(data)[2]] = data[thisSite,2:dim(data)[2]]
colnames(fullDatesDF) = colnames(data)
# create grouping variable
weekID = rep(1:length(weekDates),each=7)
weekID = weekID[1:dim(fullDatesDF)[1]]
fullDatesDF$WeekID = weekID
# Sum presence in each week
summaryData = fullDatesDF %>%
group_by(WeekID) %>%
summarize(Pres=sum(Pres,na.rm=TRUE))
# normalize by effort
effDF = data.frame(count=rep(1,length(allDates)),weekID=weekID)
effDF$count[which(is.na(fullDatesDF$Pres))] = 0
propEff = effDF %>%
group_by(weekID) %>%
summarize(sum(count))
summaryData$propEff = unlist(propEff[,2])/7
summaryData$Pres = summaryData$Pres*(1/summaryData$propEff)
summaryData$Site = sites[l]
summaryData$WeekDate = weekDates
for (j in 4:(dim(data)[2])){
var = names(data)[j]
# calculate weekly average for this covar
eval(parse(text=paste('thisCovar=fullDatesDF%>%group_by(WeekID)%>%summarize(',var,'=mean(',var,',na.rm=TRUE))',sep="")))
eval(parse(text='summaryData[[var]]=unlist(thisCovar[,2])'))
}
weeklyDF = rbind(weeklyDF,summaryData)
}
masterDF_plusJax$Mb = weeklyDF$Pres
spec = 'SpermWhale'
outDir = "E:/Chpt_3/GAM_Output"
# if it doesn't already exist, create directory to save models and figures
if (!dir.exists(paste(outDir,'/',spec,sep=""))){
dir.create(paste(outDir,'/',spec,sep=""))
}
data = data.frame(read.csv('SpermWhale_masterDF_plusJAX.csv'))
# Round presence to get Poisson dist
data$Pres = round(data$Pres)
data$Date = as.Date(data$Date,origin='1970-01-01')
# create weekly time series to reduce autocorrelation
stDt = as.Date("2016-05-01")
edDt = as.Date("2019-04-30")
sites = c('HZ','OC','NC','BC','WC','NFC','HAT','GS','BP','BS','JAX')
allDates = seq.Date(stDt,edDt,by=1)
weekDates = seq.Date(stDt,edDt,by=7)
weeklyDF = as.numeric()
for (l in 1:length(sites)) {
# Create dataframe to hold data (or NAs) for all dates
fullDatesDF = data.frame(matrix(nrow=length(allDates), ncol=dim(data)[2]))
fullDatesDF[,1] = allDates
# sort the observations we have for this site into the full date range
thisSite = which(!is.na(str_match(data$Site,sites[l])))
matchRow = match(data$Date[thisSite],allDates)
fullDatesDF[matchRow,2:dim(data)[2]] = data[thisSite,2:dim(data)[2]]
colnames(fullDatesDF) = colnames(data)
# create grouping variable
weekID = rep(1:length(weekDates),each=7)
weekID = weekID[1:dim(fullDatesDF)[1]]
fullDatesDF$WeekID = weekID
# Sum presence in each week
summaryData = fullDatesDF %>%
group_by(WeekID) %>%
summarize(Pres=sum(Pres,na.rm=TRUE))
# normalize by effort
effDF = data.frame(count=rep(1,length(allDates)),weekID=weekID)
effDF$count[which(is.na(fullDatesDF$Pres))] = 0
propEff = effDF %>%
group_by(weekID) %>%
summarize(sum(count))
summaryData$propEff = unlist(propEff[,2])/7
summaryData$Pres = summaryData$Pres*(1/summaryData$propEff)
summaryData$Site = sites[l]
summaryData$WeekDate = weekDates
for (j in 4:(dim(data)[2])){
var = names(data)[j]
# calculate weekly average for this covar
eval(parse(text=paste('thisCovar=fullDatesDF%>%group_by(WeekID)%>%summarize(',var,'=mean(',var,',na.rm=TRUE))',sep="")))
eval(parse(text='summaryData[[var]]=unlist(thisCovar[,2])'))
}
weeklyDF = rbind(weeklyDF,summaryData)
}
masterDF_plusJax$Pm = weeklyDF$Pres
spec = 'True'
outDir = "E:/Chpt_3/GAM_Output"
# if it doesn't already exist, create directory to save models and figures
if (!dir.exists(paste(outDir,'/',spec,sep=""))){
dir.create(paste(outDir,'/',spec,sep=""))
}
data = data.frame(read.csv('True_masterDF_plusJAX.csv'))
# Round presence to get Poisson dist
data$Pres = round(data$Pres)
data$Date = as.Date(data$Date,origin='1970-01-01')
# create weekly time series to reduce autocorrelation
stDt = as.Date("2016-05-01")
edDt = as.Date("2019-04-30")
sites = c('HZ','OC','NC','BC','WC','NFC','HAT','GS','BP','BS','JAX')
allDates = seq.Date(stDt,edDt,by=1)
weekDates = seq.Date(stDt,edDt,by=7)
weeklyDF = as.numeric()
for (l in 1:length(sites)) {
# Create dataframe to hold data (or NAs) for all dates
fullDatesDF = data.frame(matrix(nrow=length(allDates), ncol=dim(data)[2]))
fullDatesDF[,1] = allDates
# sort the observations we have for this site into the full date range
thisSite = which(!is.na(str_match(data$Site,sites[l])))
matchRow = match(data$Date[thisSite],allDates)
fullDatesDF[matchRow,2:dim(data)[2]] = data[thisSite,2:dim(data)[2]]
colnames(fullDatesDF) = colnames(data)
# create grouping variable
weekID = rep(1:length(weekDates),each=7)
weekID = weekID[1:dim(fullDatesDF)[1]]
fullDatesDF$WeekID = weekID
# Sum presence in each week
summaryData = fullDatesDF %>%
group_by(WeekID) %>%
summarize(Pres=sum(Pres,na.rm=TRUE))
# normalize by effort
effDF = data.frame(count=rep(1,length(allDates)),weekID=weekID)
effDF$count[which(is.na(fullDatesDF$Pres))] = 0
propEff = effDF %>%
group_by(weekID) %>%
summarize(sum(count))
summaryData$propEff = unlist(propEff[,2])/7
summaryData$Pres = summaryData$Pres*(1/summaryData$propEff)
summaryData$Site = sites[l]
summaryData$WeekDate = weekDates
for (j in 4:(dim(data)[2])){
var = names(data)[j]
# calculate weekly average for this covar
eval(parse(text=paste('thisCovar=fullDatesDF%>%group_by(WeekID)%>%summarize(',var,'=mean(',var,',na.rm=TRUE))',sep="")))
eval(parse(text='summaryData[[var]]=unlist(thisCovar[,2])'))
}
weeklyDF = rbind(weeklyDF,summaryData)
}
masterDF_plusJax$Mm = weeklyDF$Pres
colnames(masterDF_plusJax)
colnames(masterDF_plusJax) = c('WeekDate','Site','AEddyDist0','CEddyDist0','Chl0','EKE0','FSLE0','Sal0','SSH0','Temp0','VelAsp0','VelMag0','Md','Zc','Me','Kg','Gg','Dd','Gm','Mb','Pm','Mm')
colnames(masterDF_plusJax)
write.csv(masterDF_plusJax,'C:/Users/rec297/Documents/GitHub/HabitatModeling/MasterWeeklyDF_plusJAX.csv',row.names=FALSE)
modFam = tw
masterDF = data.frame(read.csv("MasterWeeklyDF_plusJAX.csv"))
View(masterDF)
View(masterDF)
