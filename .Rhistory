)+scale_color_identity(
)+scale_fill_identity(
)+guides(fill=FALSE,color=FALSE
)+theme_void(
)+ylim(c(1,4.5)
)+geom_text(aes(x=degree,
y=4.5,
label=label))
grid.arrange(plotList$Pres,plotList[[profileName]],plotList[["ColorLegend"]],
ncol=12,nrow=3,top=paste('Cuvier at',sites[j]),layout_matrix=rbind(c(rep(1,11),NA),
c(rep(2,11),NA),
c(rep(2,11),3)))
} else {
plotList[['Pres']] = ggplot(plotDF,aes(x=Date) # plot presence in this year
)+geom_line(aes(y=Pres)
)+scale_x_continuous(breaks=c(seq.Date(from=st,to=ed,by="quarter"))
)+coord_cartesian(xlim=c(st,ed)
)+ggtitle('Presence'
)+labs(x=NULL,y=NULL
)+theme(plot.margin=margin(c(0,40,0,0),unit="pt"))
plotList[[profileName]] = ggplot(profileDF,aes(x=x,y=y,z=z) # plot profile for this year
)+geom_contour_filled(
)+coord_cartesian(xlim=c(st,ed)
)+scale_x_continuous(breaks=c(seq.Date(from=st,to=ed,by="quarter"))
)+scale_y_reverse(
)+ggtitle(profileName
)+labs(x=NULL,y=NULL
)+theme(plot.margin=margin(c(0,0,0,0),unit="pt"),
legend.position="right",
legend.title=element_text(size=0),
legend.text=element_text(size=7),
legend.key.size=unit(0.5,'lines'),
legend.direction="vertical",
legend.margin=margin(0))
grid.arrange(plotList$Pres,plotList[[profileName]],
ncol=12,nrow=3,top=paste('Cuvier at',sites[j]),layout_matrix=rbind(c(rep(1,12)),
c(rep(2,12)),
c(rep(2,12))))
}
}
}
}
}
install.packages("MuMln")
install.packages("installr")
library(installr)
updateR()
data = data.frame(read.csv('J:/Chpt_3/ModelData/UD28_masterDF.csv'))
View(data)
range(data$VelMag0)
setwd("D:/Code/HabitatModeling")
library(tidyverse)
library(stringr)
library(mgcv.helper)
library(splines)
library(splines2)
library(mgcv)
library(MuMIn)
library(gratia)
library(forecast)
library(nlme)
library(itsadug)
library(AER)
## GAM approach ---------------------
# Regional model
spec = 'Blainville'
data = data.frame(read.csv('Blainville_masterDF.csv'))
# Round presence to get Poisson dist
data$Pres = round(data$Pres)
# create weekly time series to reduce autocorrelation
stDt = as.Date("2016-05-01")
edDt = as.Date("2019-04-30")
sites = c('HZ','OC','NC','BC','WC','NFC','HAT','GS','BP','BS')
allDates = stDt:edDt
weekDates = seq.Date(stDt,edDt,by=7)
weeklyDF = as.numeric()
l=1
# Create dataframe to hold data (or NAs) for all dates
fullDatesDF = data.frame(matrix(nrow=length(allDates), ncol=44))
fullDatesDF[,1] = allDates
# sort the observations we have for this site into the full date range
thisSite = which(!is.na(str_match(data$Site,sites[l])))
matchRow = match(data$Date[thisSite],allDates)
fullDatesDF[matchRow,2:dim(data)[2]] = data[thisSite,2:dim(data)[2]]
colnames(fullDatesDF) = colnames(data)
# create grouping variable
weekID = rep(1:length(weekDates),each=7)
weekID = weekID[1:dim(fullDatesDF)[1]]
fullDatesDF$WeekID = weekID
View(fullDatesDF)
# sum presence in each week
summaryData = fullDatesDF %>%
group_by(WeekID) %>%
summarize(Pres=sum(Pres,na.rm=TRUE))
View(summaryData)
# create weekly time series to reduce autocorrelation
stDt = as.Date("2016-05-01")
edDt = as.Date("2019-04-30")
sites = c('HZ','OC','NC','BC','WC','NFC','HAT','GS','BP','BS')
allDates = stDt:edDt
weekDates = seq.Date(stDt,edDt,by=7)
weeklyDF = as.numeric()
for (l in 1:length(sites)) {
# Create dataframe to hold data (or NAs) for all dates
fullDatesDF = data.frame(matrix(nrow=length(allDates), ncol=44))
fullDatesDF[,1] = allDates
# sort the observations we have for this site into the full date range
thisSite = which(!is.na(str_match(data$Site,sites[l])))
matchRow = match(data$Date[thisSite],allDates)
fullDatesDF[matchRow,2:dim(data)[2]] = data[thisSite,2:dim(data)[2]]
colnames(fullDatesDF) = colnames(data)
# create grouping variable
weekID = rep(1:length(weekDates),each=7)
weekID = weekID[1:dim(fullDatesDF)[1]]
fullDatesDF$WeekID = weekID
# sum presence in each week
summaryData = fullDatesDF %>%
group_by(WeekID) %>%
summarize(Pres=sum(Pres,na.rm=TRUE))
summaryData = fullDatesDF |>
group_by(WeekID)|>
summarize(Pres=sum(Pres,na.rm=TRUE))
# normalize by effort
effDF = data.frame(count=rep(1,length(allDates)),weekID=weekID)
effDF$count[which(is.na(fullDatesDF$Pres))] = 0
propEff = effDF %>%
group_by(weekID) %>%
summarize(sum(count))
summaryData$propEff = unlist(propEff[,2])/7
summaryData$Pres = summaryData$Pres*(1/summaryData$propEff)
summaryData$Site = sites[l]
summaryData$WeekDate = weekDates
for (j in 4:(dim(data)[2])){
var = names(data)[j]
# calculate weekly average for this covar
eval(parse(text=paste('thisCovar=fullDatesDF%>%group_by(WeekID)%>%summarize(',var,'=mean(',var,',na.rm=TRUE))',sep="")))
eval(parse(text='summaryData[[var]]=unlist(thisCovar[,2])'))
}
weeklyDF = rbind(weeklyDF,summaryData)
}
# Remove zeros in FSLE data to prepare for later transformation
data$FSLE0[data$FSLE0==0] = NA
weeklyDF$FSLE0[weeklyDF$FSLE0==0] = NA
# Transform data to fix skew, get all predictors on a similar scale
weeklyDF$log_Chl0 = log10(weeklyDF$Chl0)
weeklyDF$log_abs_FSLE0 = log10(abs(weeklyDF$FSLE0))
weeklyDF$sqrt_CEddyDist0 = sqrt(weeklyDF$CEddyDist0)
weeklyDF$sqrt_AEddyDist0 = sqrt(weeklyDF$AEddyDist0)
weeklyDF$sqrt_VelAsp0 = sqrt(weeklyDF$VelAsp0)
weeklyDF$sqrt_VelAsp700 = sqrt(weeklyDF$VelAsp700)
weeklyDF$sqrt_EKE0 = sqrt(weeklyDF$EKE0)
weeklyDF$GSDist_div100 = weeklyDF$GSDist/100
# Remove incomplete observations (NAs in FSLE)
badRows = which(is.na(data),arr.ind=TRUE)[,1]
data = data[-badRows,]
badRows = unique(which(is.na(weeklyDF),arr.ind=TRUE)[,1])
weeklyDF = weeklyDF[-badRows,]
# re-round presence data
weeklyDF$Pres = round(weeklyDF$Pres)
load("D:/Code/HabitatModeling/Blainville_WeeklyRegionalModel.Rdata")
View(weeklyDF)
# find non-zero input data to compare to associated fitted values
pres = which(weeklyDF$Pres>0)
q=predict(optWeekMod)
fits = optWeekMod$fitted.values
library(Metrics)
install.packages("Metrics")
# calculate model predictions for input data
modPreds = predict.gam(optWeekMod,weeklyDF)
plot(q,modPreds)
# calculate mean absolute error
mae(weeklyDF$Pres[pres], modPreds[pres])
library(Metrics)
# calculate mean absolute error
mae(weeklyDF$Pres[pres], modPreds[pres])
library(MLmetrics)
install.packages("MLmetrics")
library(MLmetrics)
detach(MLmetrics)
detach("package:MLmetrics", unload = TRUE)
# calculate mean absolute percent error
mErr_perc = mape(weeklyDF$Pres[pres],modPreds[pres])
# calculate mean absolute error
mErr = mae(weeklyDF$Pres[pres], modPreds[pres])
# calculate Spearman's rank correlation
predCorr = cor(weeklyDF$Pres[pres], modPreds[pres],method="spearman")
plot(weeklyDF$Pres[pres],modPreds[pres])
plot(optWeekMod$fitted.values)
plot(weeklyDF$Pres)
plot(modPreds)
View(optWeekMod)
plot(10^modPreds)
plot(10^modPreds[pres])
# calculate model predictions for input data
modPreds = predict.gam(optWeekMod,weeklyDF,type="response")
plot(modPreds)
plot(weeklyDF$Pres)
plot(modPreds,ylim=c(0,225))
# calculate model predictions for input data
modPreds = predict.gam(optWeekMod,weeklyDF,type="response")
# calculate Spearman's rank correlation
predCorr = cor(weeklyDF$Pres[pres], modPreds[pres],method="spearman")
# calculate mean absolute error
mErr = mae(weeklyDF$Pres[pres], modPreds[pres])
# calculate mean absolute percent error
mErr_perc = mape(weeklyDF$Pres[pres],modPreds[pres])
# calculate null deviance explained
nullMod = gam(weeklyDF$Pres ~ 1)
summary(nullMod)
View(nullMod)
install.packages("utils.add")
library(utils.add)
summary(optWeekmod)
summary(optWeekMod)
deviance(optWeekMod)
deviance(nullMod)
deviance(optWeekMod)/deviance(nullMod)
summary(optWeekMod)$dev.expl
summary(optWeekMod)
plot(predict.gam(optWeekMod,weeklyDF,type="response"))
plot(predict.gam(optWeekMod,weeklyDF))
nullDev = deviance(nullMod)
# model deviance
modDev = deviance(optWeekMod)
(nullDev-modDev)/nullDev
View(nullMod)
install.packages("remotes")
remotes::install_github("edwardlavender/utils.add")
library(edwardlavender/
utils.add)
library(edwardlavender/utils.add)
library(utils.add)
optWeekMod$null.deviance
(optWeekMod$null.deviance-optWeekMod$deviance)/optWeekMod$null.deviance
optWeekMod$deviance
# null deviance
nullDev = optWeekMod$null.deviance
# get model terms
terms(optWeekMod)
# get model terms
modTerms = terms(optWeekMod)
modTerms
library(stringr)
# get model terms
thisForm = as.character(optWeekMod$formula)[3]
thisForm
startSmooth = str_locate_all(thisForm,'s\\(')[[1]][,1]
termInd = str_locate_all(thisForm,'\\+')[[1]][,1]
termInd = c(0,termInd,str_length(thisForm)+1)
allTerms = character()
for (j in 1:length(termInd)-1){
thisTerm = str_sub(thisForm,start=termInd[j]+1,end=termInd[j+1]-1)
allTerms = c(allTerms,thisTerm)
}
allTerms
i=1
allTerms[i]
redMod<-update(optSiteDayMod, . ~ . - allTerms[i],)
redMod<-update(optWeekMod, . ~ . - allTerms[i],)
family(optWeekMod)
modFam = tw
redMod<-update(optWeekMod, . ~ . - allTerms[i],)
formula(redMod)
allTerms[i]
allTerms[i]
paste(allTerms[i],collapse="-")
redMod<-eval(parse(text=paste("update(optWeekMod, . ~ . - ", paste(allTerms[i],collapse="-"), ")", sep="")))
formula(redMod)
redMod<-eval(parse(text=paste("update(optWeekMod, . ~ . - ", allTerms[i], ")", sep="")))
formla(redMod)
formula(redMod)
,type="response"
# calculate model predictions for input data
modPreds = predict.gam(optWeekMod,weeklyDF)
# calculate Spearman's rank correlation
predCorr = cor(weeklyDF$Pres[pres], modPreds[pres],method="spearman")
# calculate mean absolute error
mErr = mae(weeklyDF$Pres[pres], modPreds[pres])
# calculate mean absolute percent error
mErr_perc = mape(weeklyDF$Pres[pres],modPreds[pres])
length(allTerms)
modDev = deviance(optWeekMod)
# null deviance
nullDev = optWeekMod$null.deviance
# get model terms
thisForm = as.character(optWeekMod$formula)[3]
startSmooth = str_locate_all(thisForm,'s\\(')[[1]][,1]
termInd = str_locate_all(thisForm,'\\+')[[1]][,1]
termInd = c(0,termInd,str_length(thisForm)+1)
allTerms = character()
for (j in 1:length(termInd)-1){
thisTerm = str_sub(thisForm,start=termInd[j]+1,end=termInd[j+1]-1)
allTerms = c(allTerms,thisTerm)
}
devExp = matrix(nrow=length(allTerms),ncol=1)
allTerms
for (i in 1:length(allTerms)){
redMod<-eval(parse(text=paste("update(optWeekMod, . ~ . - ", allTerms[i], ")", sep="")))
redDev = redMod$deviance
devExp[i] = (redDev-modDev)/nullDev
}
View(devExp)
(nullDev-modDev)/nullDev
nullDev
deviance(gam(data$Pres~1))
i=1
modDev
paste("update(optWeekMod, . ~ . - ", allTerms[i], ")", sep="")
redMod<-eval(parse(text=paste("update(optWeekMod, . ~ . - ", allTerms[i], ")", sep="")))
formula(redMod)
redDev = redMod$deviance
redDev
redDev-modDev
nullDev
(redDev-modDev)/nullDev
testMod = update(optWeekMod,.~.-Temp700)
formula(testMod)
allTerms
rownames(devExp)=c("FSLE0","VelAsp0","Temp0","Temp700","Sal0","Intercept")
View(devExp)
devExp = matrix(nrow=length(allTerms),ncol=1)
for (i in 1:length(allTerms)){
redMod<-eval(parse(text=paste("update(optWeekMod, . ~ . - ", allTerms[i], ")", sep="")))
redDev = redMod$deviance
devExp[i] = ((redDev-modDev)/nullDev)*100
}
rownames(devExp)=c("FSLE0","VelAsp0","Temp0","Temp700","Sal0","Intercept")
sum(devExp)
devExp = matrix(nrow=length(allTerms),ncol=1)
for (i in 1:length(allTerms)){
redMod<-eval(parse(text=paste("update(optWeekMod, . ~ . - ", allTerms[i], ")", sep="")))
redDev = redMod$deviance
devExp[i] = ((redDev-modDev)/(nullDev-modDev))*100
}
rownames(devExp)=c("FSLE0","VelAsp0","Temp0","Temp700","Sal0","Intercept")
sum(devExp)
View(optWeekMod)
redMod = optWeekMod
View(redMod)
redMod$coefficients[3:6] = 0
View(redMod)
deviance(optWeekMod)
deviance(redMod)
formula(optWeekMod)
# redMod<-eval(parse(text=paste("update(optWeekMod, . ~ . - ", allTerms[i], ")", sep="")))
redMod = eval(parse(text=paste("update(optWeekMod,.~",allTerms[i],")",sep="")))
formula(redMod)
i=1
# redMod<-eval(parse(text=paste("update(optWeekMod, . ~ . - ", allTerms[i], ")", sep="")))
redMod = eval(parse(text=paste("update(optWeekMod,.~",allTerms[i],")",sep="")))
formula(redMod)
redDev = redMod$deviance
((redDev-modDev)/(nullDev))*100
redMod<-eval(parse(text=paste("update(optWeekMod, . ~ . - ", allTerms[i], ")", sep="")))
formula(redMod)
# redMod = eval(parse(text=paste("update(optWeekMod,.~",allTerms[i],")",sep="")))
redDev = redMod$deviance
View(redMod)
View(optWeekMod)
masterDF = data.frame(read.csv("MasterWeeklyDF.csv"))
View(masterDF)
fileList = dir('J:/Chpt_3/ModelData')
fileList = dir('J:/Chpt_3/ModelData',pattern=".csv")
fileList
covars = c("Chl0","EKE0","FSLE0","SSH0","Temp0","Temp200","Temp400","Temp700","Sal0","Sal200","Sal400","Sal700",
"VelMag0","VelAsp0","CEddyDist0","AEddyDist0")
# specs = cbind(c("Blainville","Cuvier","Gervais","Kogia","Risso","Sowerby","SpermWhale","True","UD26","UD28"),
#               c("Md","Zc","Me","Kg","Gg","Mb","Pm","Mm","Gm","Dd"))
specs = cbind(rev(c("UD28","Risso","UD26","Blainville","Gervais","Cuvier","Sowerby","True","Kogia","SpermWhale")),
rev(c("Dd","Gg","Gm","Md","Me","Zc","Mb","Mm","Kg","Pm")))
masterDF = list()
allNames = list()
i=1
data = data.frame(read.csv(fileList[i]))
thisSpec = str_remove(fileList[i],'_masterDF.csv')
specAbbrev = specs[which(str_detect(specs[,1],thisSpec)),2]
# Round presence to get Poisson dist
data$Pres = round(data$Pres)
# create weekly time series to reduce autocorrelation
stDt = as.Date("2016-05-01")
edDt = as.Date("2019-04-30")
sites = c('HZ','OC','NC','BC','WC','NFC','HAT','GS','BP','BS')
allDates = stDt:edDt
weekDates = seq.Date(stDt,edDt,by=7)
weeklyDF = as.numeric()
l=1
# Create dataframe to hold data (or NAs) for all dates
fullDatesDF = data.frame(matrix(nrow=length(allDates), ncol=44))
fullDatesDF[,1] = allDates
# sort the observations we have for this site into the full date range
thisSite = which(!is.na(str_match(data$Site,sites[l])))
matchRow = match(data$Date[thisSite],allDates)
fullDatesDF[matchRow,2:dim(data)[2]] = data[thisSite,2:dim(data)[2]]
colnames(fullDatesDF) = colnames(data)
# create grouping variable
weekID = rep(1:length(weekDates),each=7)
weekID = weekID[1:dim(fullDatesDF)[1]]
fullDatesDF$WeekID = weekID
# sum presence in each week
summaryData = fullDatesDF %>%
group_by(WeekID) %>%
summarize(Pres=sum(Pres,na.rm=TRUE))
# normalize by effort
effDF = data.frame(count=rep(1,length(allDates)),weekID=weekID)
effDF$count[which(is.na(fullDatesDF$Pres))] = 0
propEff = effDF %>%
group_by(weekID) %>%
summarize(sum(count))
summaryData$propEff = unlist(propEff[,2])/7
summaryData$Pres = summaryData$Pres*(1/summaryData$propEff)
summaryData$Site = sites[l]
summaryData$WeekDate = weekDates
fileList = dir('J:/Chpt_3/ModelData',pattern=".csv")
covars = c("Chl0","EKE0","FSLE0","SSH0","Temp0","Temp200","Temp400","Temp700","Sal0","Sal200","Sal400","Sal700",
"VelMag0","VelAsp0","CEddyDist0","AEddyDist0")
# specs = cbind(c("Blainville","Cuvier","Gervais","Kogia","Risso","Sowerby","SpermWhale","True","UD26","UD28"),
#               c("Md","Zc","Me","Kg","Gg","Mb","Pm","Mm","Gm","Dd"))
specs = cbind(rev(c("UD28","Risso","UD26","Blainville","Gervais","Cuvier","Sowerby","True","Kogia","SpermWhale")),
rev(c("Dd","Gg","Gm","Md","Me","Zc","Mb","Mm","Kg","Pm")))
masterDF = list()
allNames = list()
for (i in 1:length(fileList)){
data = data.frame(read.csv(fileList[i]))
thisSpec = str_remove(fileList[i],'_masterDF.csv')
specAbbrev = specs[which(str_detect(specs[,1],thisSpec)),2]
# Round presence to get Poisson dist
data$Pres = round(data$Pres)
# create weekly time series to reduce autocorrelation
stDt = as.Date("2016-05-01")
edDt = as.Date("2019-04-30")
sites = c('HZ','OC','NC','BC','WC','NFC','HAT','GS','BP','BS')
allDates = stDt:edDt
weekDates = seq.Date(stDt,edDt,by=7)
weeklyDF = as.numeric()
for (l in 1:length(sites)) {
# Create dataframe to hold data (or NAs) for all dates
fullDatesDF = data.frame(matrix(nrow=length(allDates), ncol=44))
fullDatesDF[,1] = allDates
# sort the observations we have for this site into the full date range
thisSite = which(!is.na(str_match(data$Site,sites[l])))
matchRow = match(data$Date[thisSite],allDates)
fullDatesDF[matchRow,2:dim(data)[2]] = data[thisSite,2:dim(data)[2]]
colnames(fullDatesDF) = colnames(data)
# create grouping variable
weekID = rep(1:length(weekDates),each=7)
weekID = weekID[1:dim(fullDatesDF)[1]]
fullDatesDF$WeekID = weekID
# sum presence in each week
summaryData = fullDatesDF %>%
group_by(WeekID) %>%
summarize(Pres=sum(Pres,na.rm=TRUE))
# normalize by effort
effDF = data.frame(count=rep(1,length(allDates)),weekID=weekID)
effDF$count[which(is.na(fullDatesDF$Pres))] = 0
propEff = effDF %>%
group_by(weekID) %>%
summarize(sum(count))
summaryData$propEff = unlist(propEff[,2])/7
summaryData$Pres = summaryData$Pres*(1/summaryData$propEff)
summaryData$Site = sites[l]
summaryData$WeekDate = weekDates
for (j in 4:(dim(data)[2])){
var = names(data)[j]
# calculate weekly average for this covar
eval(parse(text=paste('thisCovar=fullDatesDF%>%group_by(WeekID)%>%summarize(',var,'=mean(',var,',na.rm=TRUE))',sep="")))
eval(parse(text='summaryData[[var]]=unlist(thisCovar[,2])'))
}
weeklyDF = rbind(weeklyDF,summaryData)
}
if (i==1){
masterDF$WeekDate = weeklyDF$WeekDate
masterDF$Site = weeklyDF$Site
allNames = c(allNames,"WeekDate","Site")
for (j in 1:length(covars)){
masterDF[[covars[j]]] = weeklyDF[[covars[j]]]
allNames = c(allNames,covars[j])
}
}
masterDF[[specAbbrev]] = weeklyDF$Pres
allNames = c(allNames,specAbbrev)
}
masterDF = data.frame(masterDF)
View(masterDF)
write.csv(masterDF,'MasterWeeklyDF.csv',row.names=FALSE)
masterDF = data.frame(read.csv("MasterWeeklyDF.csv"))
View(masterDF)
specs = cbind(rev(c("UD28","Risso","UD26","Blainville","Gervais","Cuvier","Sowerby","True","Kogia","SpermWhale")),
rev(c("Dd","Gg","Gm","Md","Me","Zc","Mb","Mm","Kg","Pm")))
modFam = tw
length(specs)
View(specs)
dim(specs)
paste(specs[i,1],'HabMod.R')
i=1
paste(specs[i,1],'HabMod.R')
# load model
load(paste(specs[i,1],'HabMod.R',sep=""))
# load model
load(paste(specs[i,1],'HabMod.Rdata',sep=""))
paste(specs[i,1],'HabMod.Rdata',sep="")
setwd("D:/Code/HabitatModeling")
# load model
load(paste(specs[i,1],'HabMod.Rdata',sep=""))
specs[i,1]
View(specs)
# load model
load(paste(specs[i,1],'HabMod.R',sep=""))
paste(getwd(),'/',specs[i,1],'HabMod.R',sep="")
# load model
load(paste(getwd(),'/',specs[i,1],'HabMod.R',sep=""))
# load model
load(paste(getwd(),'/',specs[i,1],'HabMod.Rdata',sep=""))
